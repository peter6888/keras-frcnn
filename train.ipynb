{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras_frcnn import config, data_generators\n",
    "from keras_frcnn import losses as losses\n",
    "import keras_frcnn.roi_helpers as roi_helpers\n",
    "from keras.utils import generic_utils\n",
    "\n",
    "sys.setrecursionlimit(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n",
      "Training images per class:\n",
      "{'aeroplane': 1002,\n",
      " 'bg': 0,\n",
      " 'bicycle': 837,\n",
      " 'bird': 1271,\n",
      " 'boat': 1059,\n",
      " 'bottle': 1561,\n",
      " 'bus': 685,\n",
      " 'car': 2492,\n",
      " 'cat': 1277,\n",
      " 'chair': 3056,\n",
      " 'cow': 771,\n",
      " 'diningtable': 800,\n",
      " 'dog': 1598,\n",
      " 'horse': 803,\n",
      " 'motorbike': 801,\n",
      " 'person': 17401,\n",
      " 'pottedplant': 1202,\n",
      " 'sheep': 1084,\n",
      " 'sofa': 841,\n",
      " 'train': 704,\n",
      " 'tvmonitor': 893}\n",
      "Num classes (including bg) = 21\n",
      "Config has been written to config.pickle, and can be loaded when testing to ensure correct results\n",
      "Num train samples 17125\n",
      "Num val samples 0\n",
      "loading weights from model_frcnn.hdf5\n",
      "Starting training\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 23770s - rpn_cls: 0.0869 - rpn_regr: 0.0934 - detector_cls: 1.4511 - detector_regr: 0.2266  \n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 21.137\n",
      "Classifier accuracy for bounding boxes from RPN: 0.70159375\n",
      "Loss RPN classifier: 0.09489107072852904\n",
      "Loss RPN regression: 0.09609398340294138\n",
      "Loss Detector classifier: 1.1115685375183821\n",
      "Loss Detector regression: 0.20990735262818636\n",
      "Elapsed time: 23770.283042907715\n",
      "Total loss decreased from inf to 1.5124609442780388, saving weights\n",
      "Epoch 2/10\n",
      "Average number of overlapping bounding boxes from RPN = 21.137 for 1000 previous iterations\n",
      " 120/1000 [==>...........................] - ETA: 20834s - rpn_cls: 0.0849 - rpn_regr: 0.0822 - detector_cls: 0.7980 - detector_regr: 0.1926"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f8de64b5e2ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m                                         \u001b[0msel_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                         \u001b[0mloss_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msel_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mY1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msel_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msel_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                         \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miter_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_rpn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1619\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1620\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1621\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1622\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1623\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2103\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2104\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cs231n/myVE35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "parser = OptionParser()\n",
    "\n",
    "parser.add_option(\"-p\", \"--path\", dest=\"train_path\", help=\"Path to training data.\")\n",
    "parser.add_option(\"-o\", \"--parser\", dest=\"parser\", help=\"Parser to use. One of simple or pascal_voc\",\n",
    "\t\t\t\tdefault=\"pascal_voc\")\n",
    "parser.add_option(\"-n\", \"--num_rois\", dest=\"num_rois\", help=\"Number of RoIs to process at once.\", default=32)\n",
    "parser.add_option(\"--network\", dest=\"network\", help=\"Base network to use. Supports vgg or resnet50.\", default='resnet50')\n",
    "parser.add_option(\"--hf\", dest=\"horizontal_flips\", help=\"Augment with horizontal flips in training. (Default=false).\", action=\"store_true\", default=False)\n",
    "parser.add_option(\"--vf\", dest=\"vertical_flips\", help=\"Augment with vertical flips in training. (Default=false).\", action=\"store_true\", default=False)\n",
    "parser.add_option(\"--rot\", \"--rot_90\", dest=\"rot_90\", help=\"Augment with 90 degree rotations in training. (Default=false).\",\n",
    "\t\t\t\t  action=\"store_true\", default=False)\n",
    "parser.add_option(\"--num_epochs\", dest=\"num_epochs\", help=\"Number of epochs.\", default=2000)\n",
    "parser.add_option(\"--config_filename\", dest=\"config_filename\", help=\n",
    "\t\t\t\t\"Location to store all the metadata related to the training (to be used when testing).\",\n",
    "\t\t\t\tdefault=\"config.pickle\")\n",
    "parser.add_option(\"--output_weight_path\", dest=\"output_weight_path\", help=\"Output path for weights.\", default='./model_frcnn.hdf5')\n",
    "parser.add_option(\"--input_weight_path\", dest=\"input_weight_path\", help=\"Input path for weights. If not specified, will try to load default weights provided by keras.\")\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "'''\n",
    "\n",
    "#if not options.train_path:   # if filename is not given\n",
    "#\tparser.error('Error: path to training data must be specified. Pass --path to command line')\n",
    "\n",
    "#f options.parser == 'pascal_voc':\n",
    "from keras_frcnn.pascal_voc_parser import get_data\n",
    "#elif options.parser == 'simple':\n",
    "#\tfrom keras_frcnn.simple_parser import get_data\n",
    "#else:\n",
    "#raise ValueError(\"Command line option parser must be one of 'pascal_voc' or 'simple'\")\n",
    "\n",
    "# pass the settings from the command line, and persist them in the config object\n",
    "C = config.Config()\n",
    "\n",
    "C.use_horizontal_flips = False #bool(options.horizontal_flips)\n",
    "C.use_vertical_flips = False #bool(options.vertical_flips)\n",
    "C.rot_90 = False #bool(options.rot_90)\n",
    "\n",
    "C.model_path = \"out_model_frcnn.hdf5\" #options.output_weight_path\n",
    "C.num_rois = 32 #int(options.num_rois)\n",
    "\n",
    "#if options.network == 'vgg':\n",
    "#\tC.network = 'vgg'\n",
    "#\tfrom keras_frcnn import vgg as nn\n",
    "#elif options.network == 'resnet50':\n",
    "from keras_frcnn import resnet as nn\n",
    "C.network = 'resnet50'\n",
    "#else:\n",
    "#\tprint('Not a valid model')\n",
    "#\traise ValueError\n",
    "\n",
    "\n",
    "# check if weight path was passed via command line\n",
    "#if options.input_weight_path:\n",
    "C.base_net_weights = \"model_frcnn.hdf5\" #options.input_weight_path\n",
    "#else:\n",
    "\t# set the path to weights based on backend and model\n",
    "#\tC.base_net_weights = nn.get_weight_path()\n",
    "train_path = \"data/VOCdevkit\"\n",
    "all_imgs, classes_count, class_mapping = get_data(train_path) #get_data(options.train_path)\n",
    "\n",
    "if 'bg' not in classes_count:\n",
    "\tclasses_count['bg'] = 0\n",
    "\tclass_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "C.class_mapping = class_mapping\n",
    "\n",
    "inv_map = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
    "\n",
    "config_output_filename = \"config.pickle\" #options.config_filename\n",
    "\n",
    "with open(config_output_filename, 'wb') as config_f:\n",
    "\tpickle.dump(C,config_f)\n",
    "\tprint('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))\n",
    "\n",
    "random.shuffle(all_imgs)\n",
    "\n",
    "num_imgs = len(all_imgs)\n",
    "\n",
    "train_imgs = [s for s in all_imgs if s['imageset'] == 'trainval']\n",
    "val_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n",
    "\n",
    "print('Num train samples {}'.format(len(train_imgs)))\n",
    "print('Num val samples {}'.format(len(val_imgs)))\n",
    "\n",
    "\n",
    "data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, K.image_dim_ordering(), mode='train')\n",
    "data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,K.image_dim_ordering(), mode='val')\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "\tinput_shape_img = (3, None, None)\n",
    "else:\n",
    "\tinput_shape_img = (None, None, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "\n",
    "# define the base network (resnet here, can be VGG, Inception, etc)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "try:\n",
    "\tprint('loading weights from {}'.format(C.base_net_weights))\n",
    "\tmodel_rpn.load_weights(C.base_net_weights, by_name=True)\n",
    "\tmodel_classifier.load_weights(C.base_net_weights, by_name=True)\n",
    "except:\n",
    "\tprint('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
    "\t\thttps://github.com/fchollet/keras/tree/master/keras/applications')\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')\n",
    "\n",
    "epoch_length = 1000\n",
    "num_epochs = 10 #int(options.num_epochs)\n",
    "iter_num = 0\n",
    "\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "start_time = time.time()\n",
    "\n",
    "best_loss = np.Inf\n",
    "\n",
    "class_mapping_inv = {v: k for k, v in class_mapping.items()}\n",
    "print('Starting training')\n",
    "\n",
    "vis = True\n",
    "\n",
    "for epoch_num in range(num_epochs):\n",
    "\n",
    "\tprogbar = generic_utils.Progbar(epoch_length)\n",
    "\tprint('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "\n",
    "\twhile True:\n",
    "\t\ttry:\n",
    "\n",
    "\t\t\tif len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n",
    "\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "\t\t\t\trpn_accuracy_rpn_monitor = []\n",
    "\t\t\t\tprint('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
    "\t\t\t\tif mean_overlapping_bboxes == 0:\n",
    "\t\t\t\t\tprint('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "\n",
    "\t\t\tX, Y, img_data = next(data_gen_train)\n",
    "\n",
    "\t\t\tloss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "\n",
    "\t\t\tP_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "\t\t\tR = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "\t\t\t# note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "\t\t\tX2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
    "\n",
    "\t\t\tif X2 is None:\n",
    "\t\t\t\trpn_accuracy_rpn_monitor.append(0)\n",
    "\t\t\t\trpn_accuracy_for_epoch.append(0)\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tneg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "\t\t\tpos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "\t\t\tif len(neg_samples) > 0:\n",
    "\t\t\t\tneg_samples = neg_samples[0]\n",
    "\t\t\telse:\n",
    "\t\t\t\tneg_samples = []\n",
    "\n",
    "\t\t\tif len(pos_samples) > 0:\n",
    "\t\t\t\tpos_samples = pos_samples[0]\n",
    "\t\t\telse:\n",
    "\t\t\t\tpos_samples = []\n",
    "\t\t\t\n",
    "\t\t\trpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "\t\t\trpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "\t\t\tif C.num_rois > 1:\n",
    "\t\t\t\tif len(pos_samples) < C.num_rois//2:\n",
    "\t\t\t\t\tselected_pos_samples = pos_samples.tolist()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tselected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tselected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "\t\t\t\tsel_samples = selected_pos_samples + selected_neg_samples\n",
    "\t\t\telse:\n",
    "\t\t\t\t# in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "\t\t\t\tselected_pos_samples = pos_samples.tolist()\n",
    "\t\t\t\tselected_neg_samples = neg_samples.tolist()\n",
    "\t\t\t\tif np.random.randint(0, 2):\n",
    "\t\t\t\t\tsel_samples = random.choice(neg_samples)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsel_samples = random.choice(pos_samples)\n",
    "\n",
    "\t\t\tloss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "\t\t\tlosses[iter_num, 0] = loss_rpn[1]\n",
    "\t\t\tlosses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "\t\t\tlosses[iter_num, 2] = loss_class[1]\n",
    "\t\t\tlosses[iter_num, 3] = loss_class[2]\n",
    "\t\t\tlosses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "\t\t\titer_num += 1\n",
    "\n",
    "\t\t\tprogbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "\t\t\t\t\t\t\t\t\t  ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n",
    "\n",
    "\t\t\tif iter_num == epoch_length:\n",
    "\t\t\t\tloss_rpn_cls = np.mean(losses[:, 0])\n",
    "\t\t\t\tloss_rpn_regr = np.mean(losses[:, 1])\n",
    "\t\t\t\tloss_class_cls = np.mean(losses[:, 2])\n",
    "\t\t\t\tloss_class_regr = np.mean(losses[:, 3])\n",
    "\t\t\t\tclass_acc = np.mean(losses[:, 4])\n",
    "\n",
    "\t\t\t\tmean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "\t\t\t\trpn_accuracy_for_epoch = []\n",
    "\n",
    "\t\t\t\tif C.verbose:\n",
    "\t\t\t\t\tprint('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "\t\t\t\t\tprint('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "\t\t\t\t\tprint('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "\t\t\t\t\tprint('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "\t\t\t\t\tprint('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "\t\t\t\t\tprint('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "\t\t\t\t\tprint('Elapsed time: {}'.format(time.time() - start_time))\n",
    "\n",
    "\t\t\t\tcurr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "\t\t\t\titer_num = 0\n",
    "\t\t\t\tstart_time = time.time()\n",
    "\n",
    "\t\t\t\tif curr_loss < best_loss:\n",
    "\t\t\t\t\tif C.verbose:\n",
    "\t\t\t\t\t\tprint('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "\t\t\t\t\tbest_loss = curr_loss\n",
    "\t\t\t\t\tmodel_all.save_weights(C.model_path)\n",
    "\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint('Exception: {}'.format(e))\n",
    "\t\t\tcontinue\n",
    "\n",
    "print('Training complete, exiting.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
